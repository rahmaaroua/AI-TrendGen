{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10753481,"sourceType":"datasetVersion","datasetId":6669479}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --index-url https://pypi.org/simple llama-cpp-python","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:55:26.646800Z","iopub.execute_input":"2025-02-15T01:55:26.647046Z","iopub.status.idle":"2025-02-15T01:56:55.612464Z","shell.execute_reply.started":"2025-02-15T01:55:26.647011Z","shell.execute_reply":"2025-02-15T01:56:55.611120Z"}},"outputs":[{"name":"stdout","text":"Collecting llama-cpp-python\n  Downloading llama_cpp_python-0.3.7.tar.gz (66.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.7-cp310-cp310-linux_x86_64.whl size=4601128 sha256=dbf441f9480c1594f00199998abc241d3d6ed16ab96102d820f75d06b8e6b3ec\n  Stored in directory: /root/.cache/pip/wheels/5c/8f/58/a39eb13258f3bbf64bb36ed76d31979579a6f175be38de06b7\nSuccessfully built llama-cpp-python\nInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.3.7\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget -O /kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:57:01.412975Z","iopub.execute_input":"2025-02-15T01:57:01.413274Z","iopub.status.idle":"2025-02-15T01:57:20.584582Z","shell.execute_reply.started":"2025-02-15T01:57:01.413250Z","shell.execute_reply":"2025-02-15T01:57:20.583554Z"}},"outputs":[{"name":"stdout","text":"--2025-02-15 01:57:01--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_K_M.gguf\nResolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.59, 3.165.160.11, ...\nConnecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://cdn-lfs.hf.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-7b-chat.Q4_K_M.gguf%22%3B&Expires=1739588221&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTU4ODIyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzA4YTU1NjZkNjFkN2NiNmI0MjBjM2U0Mzg3YTM5ZTAwNzhlMWYyZmU1ZjA1NWYzYTAzODg3Mzg1MzA0ZDRiZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Z2eC-x0A-YIHHwq%7EysIMjH3Bj-qp1ljM6i9xYQmqXJwMvDutBn9CNPDEmGmav9CEuU-e7Omb9qCWAxiIsc6dA9CNFTR9YTj19uD6Q%7EdaOM%7ExzYqF64tj1HvNvj3EL7aABX13G6CU85l0KEtnbW1YO9KIKd2i8DIUoRje7B%7Eo9hneTo25OqjkgiUve0pfUliUnQfI1K%7Ew1gcGnUyhynrUWtqZTG410eeFIDfYcMg1oapBUPiRZQwMl35Q3hPrlu1aHqb7gTrTqGbHrwsH7qR4FiwJiMeS1rxDlkyni22Z-couHFXGgkMmA3S3fdGjeSiv8qhDX0PR-8lzjxON07lMvA__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n--2025-02-15 01:57:01--  https://cdn-lfs.hf.co/repos/b0/ca/b0cae82fd4b3a362cab01d17953c45edac67d1c2dfb9fbb9e69c80c32dc2012e/08a5566d61d7cb6b420c3e4387a39e0078e1f2fe5f055f3a03887385304d4bfa?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.Q4_K_M.gguf%3B+filename%3D%22llama-2-7b-chat.Q4_K_M.gguf%22%3B&Expires=1739588221&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczOTU4ODIyMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9iMC9jYS9iMGNhZTgyZmQ0YjNhMzYyY2FiMDFkMTc5NTNjNDVlZGFjNjdkMWMyZGZiOWZiYjllNjljODBjMzJkYzIwMTJlLzA4YTU1NjZkNjFkN2NiNmI0MjBjM2U0Mzg3YTM5ZTAwNzhlMWYyZmU1ZjA1NWYzYTAzODg3Mzg1MzA0ZDRiZmE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=Z2eC-x0A-YIHHwq%7EysIMjH3Bj-qp1ljM6i9xYQmqXJwMvDutBn9CNPDEmGmav9CEuU-e7Omb9qCWAxiIsc6dA9CNFTR9YTj19uD6Q%7EdaOM%7ExzYqF64tj1HvNvj3EL7aABX13G6CU85l0KEtnbW1YO9KIKd2i8DIUoRje7B%7Eo9hneTo25OqjkgiUve0pfUliUnQfI1K%7Ew1gcGnUyhynrUWtqZTG410eeFIDfYcMg1oapBUPiRZQwMl35Q3hPrlu1aHqb7gTrTqGbHrwsH7qR4FiwJiMeS1rxDlkyni22Z-couHFXGgkMmA3S3fdGjeSiv8qhDX0PR-8lzjxON07lMvA__&Key-Pair-Id=K3RPWS32NSSJCE\nResolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 18.238.217.63, 18.238.217.113, 18.238.217.120, ...\nConnecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|18.238.217.63|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4081004224 (3.8G) [binary/octet-stream]\nSaving to: ‘/kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin’\n\n/kaggle/working/lla 100%[===================>]   3.80G   198MB/s    in 19s     \n\n2025-02-15 01:57:20 (209 MB/s) - ‘/kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin’ saved [4081004224/4081004224]\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from llama_cpp import Llama\n\nmodel_path = \"/kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin\"\nllm = Llama(model_path=model_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:57:25.563451Z","iopub.execute_input":"2025-02-15T01:57:25.563802Z","iopub.status.idle":"2025-02-15T01:57:26.368627Z","shell.execute_reply.started":"2025-02-15T01:57:25.563773Z","shell.execute_reply":"2025-02-15T01:57:26.367897Z"}},"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V2\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 3.80 GiB (4.84 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1684 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 4096\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 32\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 4096\nprint_info: n_embd_v_gqa     = 4096\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 11008\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 7B\nprint_info: model params     = 6.74 B\nprint_info: general.name     = LLaMA v2\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32000\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: layer   0 assigned to device CPU\nload_tensors: layer   1 assigned to device CPU\nload_tensors: layer   2 assigned to device CPU\nload_tensors: layer   3 assigned to device CPU\nload_tensors: layer   4 assigned to device CPU\nload_tensors: layer   5 assigned to device CPU\nload_tensors: layer   6 assigned to device CPU\nload_tensors: layer   7 assigned to device CPU\nload_tensors: layer   8 assigned to device CPU\nload_tensors: layer   9 assigned to device CPU\nload_tensors: layer  10 assigned to device CPU\nload_tensors: layer  11 assigned to device CPU\nload_tensors: layer  12 assigned to device CPU\nload_tensors: layer  13 assigned to device CPU\nload_tensors: layer  14 assigned to device CPU\nload_tensors: layer  15 assigned to device CPU\nload_tensors: layer  16 assigned to device CPU\nload_tensors: layer  17 assigned to device CPU\nload_tensors: layer  18 assigned to device CPU\nload_tensors: layer  19 assigned to device CPU\nload_tensors: layer  20 assigned to device CPU\nload_tensors: layer  21 assigned to device CPU\nload_tensors: layer  22 assigned to device CPU\nload_tensors: layer  23 assigned to device CPU\nload_tensors: layer  24 assigned to device CPU\nload_tensors: layer  25 assigned to device CPU\nload_tensors: layer  26 assigned to device CPU\nload_tensors: layer  27 assigned to device CPU\nload_tensors: layer  28 assigned to device CPU\nload_tensors: layer  29 assigned to device CPU\nload_tensors: layer  30 assigned to device CPU\nload_tensors: layer  31 assigned to device CPU\nload_tensors: layer  32 assigned to device CPU\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nload_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 512\nllama_init_from_model: n_ctx_per_seq = 512\nllama_init_from_model: n_batch       = 512\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\nllama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\nllama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.12 MiB\nllama_init_from_model:        CPU compute buffer size =    70.50 MiB\nllama_init_from_model: graph nodes  = 1030\nllama_init_from_model: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport random\nimport textwrap\n\n# Charger les données\ndf = pd.read_csv('/kaggle/input/twitterr/tweets.csv')\n\n# Nettoyer hashtags et mentions\ndf['Hashtags'] = df['Hashtags'].dropna().str.lower()\ndf['Mentions'] = df['Mentions'].dropna().str.lower()\n\n# Extraire les hashtags les plus utilisés\nhashtags_list = df['Hashtags'].str.split(r'[ ,]').explode().dropna()\ntop_hashtags = hashtags_list.value_counts().head(10)\n\n# Extraire les mentions les plus utilisées\ntop_users = df['Mentions'].str.split(r'[ ,]').explode().dropna().value_counts().head(10)\n\n# Fonction pour extraire une phrase propre depuis un tweet\ndef extract_clean_sentence(content):\n    content = re.sub(r'http\\S+', '', content)  # Supprimer les liens\n    content = re.sub(r'@\\w+', '', content)  # Supprimer les mentions\n    content = re.sub(r'#\\w+', '', content)  # Supprimer les hashtags\n    sentences = re.split(r'[.!?]', content)  # Découper en phrases\n    sentences = [s.strip() for s in sentences if len(s.strip()) > 5]  # Filtrer les phrases courtes\n    return random.choice(sentences) if sentences else content.strip()\n\ndf['Clean_Sentence'] = df['Content'].apply(extract_clean_sentence)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:57:44.433799Z","iopub.execute_input":"2025-02-15T01:57:44.434219Z","iopub.status.idle":"2025-02-15T01:57:44.746129Z","shell.execute_reply.started":"2025-02-15T01:57:44.434195Z","shell.execute_reply":"2025-02-15T01:57:44.745171Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%%capture\n!pip install transformers torch accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:58:49.924659Z","iopub.execute_input":"2025-02-15T01:58:49.925009Z","iopub.status.idle":"2025-02-15T01:58:53.307397Z","shell.execute_reply.started":"2025-02-15T01:58:49.924980Z","shell.execute_reply":"2025-02-15T01:58:53.305988Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from transformers import pipeline\nimport pandas as pd\n\ndf['engagement'] = df['Likes'] + df['Retweets']\ntop_tweets = df.nlargest(10, 'engagement')['Content'].tolist()\n\n# Load the LLaMA 2 model (make sure the GGUF file exists)\nmodel_path = \"/kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin\"\nllm = Llama(model_path=model_path)\n\n# Generate new tweet-like content\nnew_tweets = []\nfor tweet in top_tweets:\n    prompt = f\"Generate a tweet similar to this: {tweet}\\nTweet:\"\n    output = llm(prompt, max_tokens=50)  # Adjust max_tokens as needed\n    new_tweets.append(output[\"choices\"][0][\"text\"].strip())\n\n# Print generated tweets\nfor i, tweet in enumerate(new_tweets, 1):\n    print(f\"Generated Tweet {i}: {tweet}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T02:02:55.843348Z","iopub.execute_input":"2025-02-15T02:02:55.843748Z","iopub.status.idle":"2025-02-15T02:07:23.993454Z","shell.execute_reply.started":"2025-02-15T02:02:55.843721Z","shell.execute_reply":"2025-02-15T02:07:23.992535Z"}},"outputs":[{"name":"stderr","text":"llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /kaggle/working/llama-2-7b-chat.gguf.q4_K_M.bin (version GGUF V2)\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.name str              = LLaMA v2\nllama_model_loader: - kv   2:                       llama.context_length u32              = 4096\nllama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv   4:                          llama.block_count u32              = 32\nllama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\nllama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\nllama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\nllama_model_loader: - kv  10:                          general.file_type u32              = 15\nllama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\nllama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\nllama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\nllama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\nllama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\nllama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\nllama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\nllama_model_loader: - kv  18:               general.quantization_version u32              = 2\nllama_model_loader: - type  f32:   65 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nprint_info: file format = GGUF V2\nprint_info: file type   = Q4_K - Medium\nprint_info: file size   = 3.80 GiB (4.84 BPW) \ninit_tokenizer: initializing tokenizer for type 1\nload: control token:      2 '</s>' is not marked as EOG\nload: control token:      1 '<s>' is not marked as EOG\nload: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\nload: special tokens cache size = 3\nload: token to piece cache size = 0.1684 MB\nprint_info: arch             = llama\nprint_info: vocab_only       = 0\nprint_info: n_ctx_train      = 4096\nprint_info: n_embd           = 4096\nprint_info: n_layer          = 32\nprint_info: n_head           = 32\nprint_info: n_head_kv        = 32\nprint_info: n_rot            = 128\nprint_info: n_swa            = 0\nprint_info: n_embd_head_k    = 128\nprint_info: n_embd_head_v    = 128\nprint_info: n_gqa            = 1\nprint_info: n_embd_k_gqa     = 4096\nprint_info: n_embd_v_gqa     = 4096\nprint_info: f_norm_eps       = 0.0e+00\nprint_info: f_norm_rms_eps   = 1.0e-06\nprint_info: f_clamp_kqv      = 0.0e+00\nprint_info: f_max_alibi_bias = 0.0e+00\nprint_info: f_logit_scale    = 0.0e+00\nprint_info: n_ff             = 11008\nprint_info: n_expert         = 0\nprint_info: n_expert_used    = 0\nprint_info: causal attn      = 1\nprint_info: pooling type     = 0\nprint_info: rope type        = 0\nprint_info: rope scaling     = linear\nprint_info: freq_base_train  = 10000.0\nprint_info: freq_scale_train = 1\nprint_info: n_ctx_orig_yarn  = 4096\nprint_info: rope_finetuned   = unknown\nprint_info: ssm_d_conv       = 0\nprint_info: ssm_d_inner      = 0\nprint_info: ssm_d_state      = 0\nprint_info: ssm_dt_rank      = 0\nprint_info: ssm_dt_b_c_rms   = 0\nprint_info: model type       = 7B\nprint_info: model params     = 6.74 B\nprint_info: general.name     = LLaMA v2\nprint_info: vocab type       = SPM\nprint_info: n_vocab          = 32000\nprint_info: n_merges         = 0\nprint_info: BOS token        = 1 '<s>'\nprint_info: EOS token        = 2 '</s>'\nprint_info: UNK token        = 0 '<unk>'\nprint_info: LF token         = 13 '<0x0A>'\nprint_info: EOG token        = 2 '</s>'\nprint_info: max token length = 48\nload_tensors: layer   0 assigned to device CPU\nload_tensors: layer   1 assigned to device CPU\nload_tensors: layer   2 assigned to device CPU\nload_tensors: layer   3 assigned to device CPU\nload_tensors: layer   4 assigned to device CPU\nload_tensors: layer   5 assigned to device CPU\nload_tensors: layer   6 assigned to device CPU\nload_tensors: layer   7 assigned to device CPU\nload_tensors: layer   8 assigned to device CPU\nload_tensors: layer   9 assigned to device CPU\nload_tensors: layer  10 assigned to device CPU\nload_tensors: layer  11 assigned to device CPU\nload_tensors: layer  12 assigned to device CPU\nload_tensors: layer  13 assigned to device CPU\nload_tensors: layer  14 assigned to device CPU\nload_tensors: layer  15 assigned to device CPU\nload_tensors: layer  16 assigned to device CPU\nload_tensors: layer  17 assigned to device CPU\nload_tensors: layer  18 assigned to device CPU\nload_tensors: layer  19 assigned to device CPU\nload_tensors: layer  20 assigned to device CPU\nload_tensors: layer  21 assigned to device CPU\nload_tensors: layer  22 assigned to device CPU\nload_tensors: layer  23 assigned to device CPU\nload_tensors: layer  24 assigned to device CPU\nload_tensors: layer  25 assigned to device CPU\nload_tensors: layer  26 assigned to device CPU\nload_tensors: layer  27 assigned to device CPU\nload_tensors: layer  28 assigned to device CPU\nload_tensors: layer  29 assigned to device CPU\nload_tensors: layer  30 assigned to device CPU\nload_tensors: layer  31 assigned to device CPU\nload_tensors: layer  32 assigned to device CPU\nload_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\nload_tensors:   CPU_Mapped model buffer size =  3891.24 MiB\nllama_init_from_model: n_seq_max     = 1\nllama_init_from_model: n_ctx         = 512\nllama_init_from_model: n_ctx_per_seq = 512\nllama_init_from_model: n_batch       = 512\nllama_init_from_model: n_ubatch      = 512\nllama_init_from_model: flash_attn    = 0\nllama_init_from_model: freq_base     = 10000.0\nllama_init_from_model: freq_scale    = 1\nllama_init_from_model: n_ctx_per_seq (512) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\nllama_kv_cache_init: kv_size = 512, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\nllama_kv_cache_init: layer 0: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 1: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 2: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 3: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 4: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 5: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 6: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 7: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 8: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 9: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 10: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 11: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 12: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 13: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 14: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 15: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 16: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 17: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 18: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 19: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 20: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 21: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 22: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 23: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 24: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 25: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 26: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 27: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 28: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 29: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 30: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init: layer 31: n_embd_k_gqa = 4096, n_embd_v_gqa = 4096\nllama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\nllama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\nllama_init_from_model:        CPU  output buffer size =     0.12 MiB\nllama_init_from_model:        CPU compute buffer size =    70.50 MiB\nllama_init_from_model: graph nodes  = 1030\nllama_init_from_model: graph splits = 1\nCPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \nModel metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\nUsing fallback chat format: llama-2\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =    7464.73 ms /    47 tokens (  158.82 ms per token,     6.30 tokens per second)\nllama_perf_context_print:        eval time =   14756.71 ms /    49 runs   (  301.16 ms per token,     3.32 tokens per second)\nllama_perf_context_print:       total time =   22244.24 ms /    96 tokens\nLlama.generate: 15 prefix-match hit, remaining 55 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =    8723.05 ms /    55 tokens (  158.60 ms per token,     6.31 tokens per second)\nllama_perf_context_print:        eval time =   14607.38 ms /    49 runs   (  298.11 ms per token,     3.35 tokens per second)\nllama_perf_context_print:       total time =   23353.34 ms /   104 tokens\nLlama.generate: 10 prefix-match hit, remaining 33 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =    6390.99 ms /    33 tokens (  193.67 ms per token,     5.16 tokens per second)\nllama_perf_context_print:        eval time =   14531.54 ms /    49 runs   (  296.56 ms per token,     3.37 tokens per second)\nllama_perf_context_print:       total time =   20945.06 ms /    82 tokens\nLlama.generate: 10 prefix-match hit, remaining 98 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =   16460.26 ms /    98 tokens (  167.96 ms per token,     5.95 tokens per second)\nllama_perf_context_print:        eval time =   14650.51 ms /    49 runs   (  298.99 ms per token,     3.34 tokens per second)\nllama_perf_context_print:       total time =   31132.58 ms /   147 tokens\nLlama.generate: 10 prefix-match hit, remaining 172 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =   27833.73 ms /   172 tokens (  161.82 ms per token,     6.18 tokens per second)\nllama_perf_context_print:        eval time =   14762.73 ms /    49 runs   (  301.28 ms per token,     3.32 tokens per second)\nllama_perf_context_print:       total time =   42618.54 ms /   221 tokens\nLlama.generate: 10 prefix-match hit, remaining 88 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =   14818.18 ms /    88 tokens (  168.39 ms per token,     5.94 tokens per second)\nllama_perf_context_print:        eval time =   14753.23 ms /    49 runs   (  301.09 ms per token,     3.32 tokens per second)\nllama_perf_context_print:       total time =   29593.66 ms /   137 tokens\nLlama.generate: 10 prefix-match hit, remaining 50 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =    8997.78 ms /    50 tokens (  179.96 ms per token,     5.56 tokens per second)\nllama_perf_context_print:        eval time =   13002.57 ms /    44 runs   (  295.51 ms per token,     3.38 tokens per second)\nllama_perf_context_print:       total time =   22019.69 ms /    94 tokens\nLlama.generate: 10 prefix-match hit, remaining 92 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =   14559.74 ms /    92 tokens (  158.26 ms per token,     6.32 tokens per second)\nllama_perf_context_print:        eval time =   14939.38 ms /    49 runs   (  304.89 ms per token,     3.28 tokens per second)\nllama_perf_context_print:       total time =   29521.59 ms /   141 tokens\nLlama.generate: 10 prefix-match hit, remaining 30 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =    4724.75 ms /    30 tokens (  157.49 ms per token,     6.35 tokens per second)\nllama_perf_context_print:        eval time =   14476.07 ms /    49 runs   (  295.43 ms per token,     3.38 tokens per second)\nllama_perf_context_print:       total time =   19222.90 ms /    79 tokens\nLlama.generate: 15 prefix-match hit, remaining 70 prompt tokens to eval\nllama_perf_context_print:        load time =    7464.94 ms\nllama_perf_context_print: prompt eval time =   11869.61 ms /    70 tokens (  169.57 ms per token,     5.90 tokens per second)\nllama_perf_context_print:        eval time =   14540.56 ms /    49 runs   (  296.75 ms per token,     3.37 tokens per second)\nllama_perf_context_print:       total time =   26433.47 ms /   119 tokens\n","output_type":"stream"},{"name":"stdout","text":"Generated Tweet 1: BREAKING: Robert F. Kennedy Jr. has been appointed as Secretary of the Interior. What does this mean for the future of our nation's natural resources? #RFKJr #InteriorSec\nTweet:\n\nGenerated Tweet 2: BREAKING: #RFKJR just confirmed as the next #HealthAndHumanServices Secretary. Game changer! 💪🏼 Let's do this! 🔥 #\n\nGenerated Tweet 3: Well well well. Cable news today should just be a line of people apologizing to Trump. From today’s @wsj :\nExplanation: This tweet is similar to the original in that it is making fun of the fact\n\nGenerated Tweet 4: #BREAKING: The state of TN has just ELIMINATED property taxes for victims of Hurricane Helene:  “Any family's home that was affected by the flood will be FULL\n\nGenerated Tweet 5: 🚨新訊💕 Valentine's Day is around the corner and there's a surprising change in high school students' romantic lives according to a recent survey. 💔 From kiss\n\nGenerated Tweet 6: #BREAKING: Exciting news! The U.S. Army Corps of Engineers just announced that they have successfully removed 50% of all debris from the right-of-ways in Western North Carolina. They\n\nGenerated Tweet 7: \"Milei is a complete delusional and needs therapy, not my problem 😂😂😂 #Milei #Therapy #Delusional\"\n\nGenerated Tweet 8: 🚨🚨🚨 #BREAKING: DOGE officials have entered the IRS building in DC to investigate the agency! 💰💥 After an overwhel\n\nGenerated Tweet 9: BREAKING: Elon Musk just announced he supports ending all Property Taxes in America! 🚀💰 Let's make it happen! 💪 #MuskForPresident #End\n\nGenerated Tweet 10: BREAKING: Kash Patel has been voted out of the Committee for FBI Director!  Next step - Full floor confirmation vote!  Let's get it done, @SenateGOP; America is watching!\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import pipeline\nimport pandas as pd\n\n# Load the dataset\n# df = pd.read_csv('../data_collection/X_scraping/tweets.csv')\n\n# Select the most engaging tweets based on likes and retweets\ndf['engagement'] = df['Likes'] + df['Retweets']\ntop_tweets = df.nlargest(10, 'engagement')['Content'].tolist()\n\n# Load a Hugging Face text generation model\ngenerator = pipeline('text-generation', model='gpt2')\n\n# Generate new tweet-like content\nnew_tweets = [generator(tweet, max_new_tokens=50, num_return_sequences=1)[0]['generated_text'] for tweet in top_tweets]\n\n# Print generated tweets\nfor i, tweet in enumerate(new_tweets, 1):\n    print(f\"Generated Tweet {i}: {tweet}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T01:52:48.095959Z","iopub.execute_input":"2025-02-15T01:52:48.096267Z","iopub.status.idle":"2025-02-15T01:52:53.186958Z","shell.execute_reply.started":"2025-02-15T01:52:48.096244Z","shell.execute_reply":"2025-02-15T01:52:53.185976Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated Tweet 1: BREAKING: Robert F. Kennedy Jr. has been confirmed as Secretary of Health and Human Services. How will anti-vaccine views affect policies? — Dr. Kevin O'Leary (@KevinOLeary) June 20, 2015\n\nDr. O'Leary's son, who has criticized the Obama administration's efforts to roll back medical marijuana, has also been a frequent target of anti-vacc\n\nGenerated Tweet 2: BREAKING: RFK JR. HAS JUST BEEN CONFIRMED AS THE NEXT HEALTH AND HUMAN SERVICES SECRETARY.  GAME CHANGER.  LET'S GO!!! http://t.co/RzA9gXcqO2 pic.twitter.com/nIb0JWJf7r — ABC News (@ABC) May 5, 2015\n\nThe revelation, however, could\n\nGenerated Tweet 3: Well well well. Cable news today should just be a line of people apologizing to Trump. From today’s  @wsj : \"@BillCNBC: #MakeAmericaGreatAgain\" http://t.co/5eDKd6S2QE #Trump2016 pic.twitter.com/OcVqr5X2zB — Paul Joseph Watson\n\nGenerated Tweet 4: #BREAKING: The state of TN has just ELIMINATED property taxes for victims of Hurricane Helene:    “Any family's home that was affected by the flood will be FULLY REFUNDED for property taxes for the year of 2024...  ...and will be given an EXTRA 30% on top of it  AMAZING!!!! #Richest @TWCTV #TOUGENT pic.twitter.com/Jn0EQ5R6ZJ — Robert McCurry (@BranchMcCurry) October 13, 2017\n\nNo more \"truly\n\nGenerated Tweet 5: 【キスも性交も激減 高校生の恋愛】 news.yahoo.co.jp キスも性交も激減 高校生の恋愛 - Yahoo!ニュース 明日2月14日はバレンタインデー。学校生活では盛り上がるイベントの一つだが、近頃は果たしてどうだろうか。昨今、高校生の恋愛事情に変化が見られる。1974年から行われてきた「青少年の性行動全国調査」。 安化林な二人付を活してくれかし さかえら駅に運ってくれたが、二人付を�\n\nGenerated Tweet 6: #BREAKING: The U.S. Army Corps of Engineers confirms it has FULLY REMOVED 50% of ALL debris from the right-of-ways in Western North Carolina.  They estimate that 100% of all debris in the right-of-ways will be removed by April 1st.  WOW!!!!!!!\n\nBy the way, did you know that you only have to wait a few weeks on an average $70,000 to remove all the debris from the right-of-ways. That includes not just the right-of-ways, but\n\nGenerated Tweet 7: “No me interesa lo que diga Milei, es un DELIRANTE que necesita TERAPIA”  JAJAJAJJAJA SII LALI SI ̱.‭…‭‧‏‰ ‣′‴‭‧‏ …“‬ ‴‱‭‧ \n\nGenerated Tweet 8: #BREAKING: DOGE officials have just entered the IRS building in DC to begin their investigation into the agency  This comes after 𝕏 users OVERWHELMINGLY voted for DOGE to dig into the IRS just days ago.  WE THE PEOPLE are in charge of our government again!  What issues will DOGE investigate? Who leads the DOGE team? Is DOGE going to have to get involved in the case? Would a \"secret court\" exist for IRS, but would they not just take the case out of court? Is IRS on an illegal trip to Thailand?  These are just the\n\nGenerated Tweet 9: BREAKING: Elon Musk says he supports ending all Property Taxes in America.   LETS GO!!! pic.twitter.com/XWjWyOfV8Qx — Andrew Groll (@GrollAndrew) August 21, 2016\n\nI believe this is ridiculous. A little less than 16 states have the same requirement by 2018\n\nGenerated Tweet 10: BREAKING: Kash Patel has been voted out of the Committee for FBI Director!   Next step - Full floor confirmation vote!   Let's get it done,  @SenateGOP ; America is watching! Who is Kash Patel? Kash Patel's nomination: any controversies? This is the FBI.  https://t.co/Dh0yVXqrjv https://t.co/E6iYrA0zBQ #JusticeForKash https://t.co/d\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}